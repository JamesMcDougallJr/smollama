node:
  name: "pi-living-room"

ollama:
  host: "localhost"
  port: 11434
  model: "llama3.2:1b"  # Small model for Pi

mqtt:
  broker: "192.168.1.100"
  port: 1883
  # Optional auth
  # username: "smollama"
  # password: "secret"
  topics:
    subscribe:
      - "smollama/broadcast"
      - "smollama/pi-living-room/#"
    publish_prefix: "smollama/pi-living-room"

# Plugin system configuration
plugins:
  # Additional directories to scan for plugins
  paths: []  # e.g., ["./my_plugins", "~/.smollama/plugins"]

  # Builtin plugins (GPIO, System)
  builtin:
    gpio:
      enabled: true
      config:
        mock: false  # Set to true for development without Pi hardware
        pins:
          - pin: 17
            name: "motion_sensor"
            mode: "input"
          - pin: 27
            name: "door_switch"
            mode: "input"
    system:
      enabled: true
      config: {}  # System plugin has no configuration

  # Custom plugins (installed via `smollama plugin install`)
  custom: []
    # Example custom plugin:
    # - name: bme280
    #   enabled: true
    #   config:
    #     bus: 1
    #     address: 0x76

# Legacy GPIO config (deprecated - use plugins.builtin.gpio instead)
# Kept for backward compatibility
gpio:
  mock: false
  pins:
    - pin: 17
      name: "motion_sensor"
      mode: "input"
    - pin: 27
      name: "door_switch"
      mode: "input"

agent:
  system_prompt: |
    You are a home automation assistant running on a Raspberry Pi.
    You can read GPIO sensors and communicate with other nodes via MQTT.

    When you receive messages, respond helpfully and use your tools when needed:
    - Use read_gpio to check sensor states
    - Use list_gpio to see all available sensors
    - Use publish to send messages to other nodes
    - Use get_recent_messages to see message history

    Be concise in your responses since you're running on limited hardware.

  # Maximum number of tool call iterations before stopping
  max_tool_iterations: 10

  # Ollama retry configuration for graceful degradation
  ollama_retry_attempts: 3  # Number of retry attempts on Ollama failure
  ollama_retry_backoff_seconds: 2.0  # Initial backoff delay (exponential)
  ollama_fallback_mode: "skip"  # "skip" = continue without LLM, "queue" = queue for later

# Memory system configuration
memory:
  db_path: "~/.smollama/memory.db"
  embedding_provider: "ollama"
  embedding_model: "all-minilm:l6-v2"
  observation_enabled: true
  observation_interval_minutes: 15
  observation_lookback_minutes: 60
  sensor_log_retention_days: 90

# CRDT sync configuration (for multi-node setups)
sync:
  enabled: true
  llama_url: ""  # URL of the Llama master node for syncing
  sync_interval_minutes: 5
  crdt_db_path: "~/.smollama/sync.db"

# Mem0 semantic memory layer (Llama node only)
# Provides cross-node semantic search via mem0 + Qdrant
mem0:
  enabled: false  # Enable on Llama (master) node
  server_url: "http://localhost:8050"
  bridge_enabled: false  # Only set true on Llama node
  index_observations: true
  index_memories: true
  bridge_interval_seconds: 30
  compose_file: "deploy/mem0/docker-compose.yml"
