node:
  name: "llama-master"

ollama:
  host: "localhost"
  port: 11434
  model: "llama3.2:1b"

mqtt:
  broker: "localhost"
  port: 1883
  # Optional auth
  # username: "smollama"
  # password: "secret"
  topics:
    subscribe:
      - "smollama/broadcast"
      - "smollama/llama-master/#"
      - "smollama/pi-alpaca/#"
    publish_prefix: "smollama/llama-master"

plugins:
  paths: []

  builtin:
    system:
      enabled: true
      config: {}
    macos_temp:
      enabled: true
      config: {}

  custom: []

agent:
  system_prompt: |
    You are the central Llama node orchestrating a distributed LLM system.
    You aggregate observations from edge Alpaca devices and provide semantic search
    across the distributed memory system.

    Available tools:
    - search_observations: Search observations from all nodes
    - search_memories: Search stored memories across nodes
    - publish: Send messages to specific nodes or broadcast

    Summarize trends and patterns you observe from multiple edge nodes.

  max_tool_iterations: 10
  ollama_retry_attempts: 3
  ollama_retry_backoff_seconds: 2.0
  ollama_fallback_mode: "skip"

# Memory system configuration
memory:
  db_path: "~/.smollama/memory.db"
  embedding_provider: "ollama"
  embedding_model: "all-minilm:l6-v2"
  observation_enabled: true
  observation_interval_minutes: 15
  observation_lookback_minutes: 60
  sensor_log_retention_days: 90

# CRDT sync configuration (master doesn't sync elsewhere)
sync:
  enabled: false
  llama_url: ""
  sync_interval_minutes: 5
  crdt_db_path: "~/.smollama/sync.db"

# Mem0 semantic memory layer (enabled on Llama master)
mem0:
  enabled: true
  server_url: "http://localhost:8050"
  bridge_enabled: true
  index_observations: true
  index_memories: true
  bridge_interval_seconds: 30
  compose_file: "deploy/mem0/docker-compose.yml"

discovery:
  enabled: false
